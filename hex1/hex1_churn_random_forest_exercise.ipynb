{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c4f9a44",
   "metadata": {},
   "source": [
    "# 🧠 Predicting Customer Churn Using Random Forests\n",
    "Welcome to this practical machine learning exercise. In this notebook, you'll develop a solution for predicting customer churn — a classic and highly practical problem in economics, management, and business analytics.\n",
    "\n",
    "**Goal:** Apply machine learning to identify customers at risk of leaving (churning), using historical data.\n",
    "\n",
    "Along the way, you'll:\n",
    "- Preprocess data responsibly (avoiding leakage)\n",
    "- Train and evaluate models\n",
    "- Build pipelines for cleaner workflows\n",
    "- Tune models using validation sets\n",
    "- Deploy a final product-ready solution\n",
    "\n",
    "This notebook emphasizes **intuitive understanding**, **real-world practices**, and links to **further resources**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6efa9c-5882-4b42-b50f-a4ab2d49462b",
   "metadata": {},
   "source": [
    "**Note:** Documentation links are provided extensively, to help you, in the following format:\n",
    "  - [📘 StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a09d78-c36f-4a6b-bace-983e2e2108d8",
   "metadata": {},
   "source": [
    "**Note: Whenever you see the ✍️ symbol, it means you must fill in the blanks with your own code!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba58bf0-b2e4-4a38-9be0-d3614c4ef242",
   "metadata": {},
   "source": [
    "## ⚙️ Step 0: Setup\n",
    "\n",
    "Before we begin, we load the core Python libraries used for:\n",
    "- **Data manipulation** (`pandas`, `numpy`)\n",
    "- **Visualization** (`matplotlib`, `seaborn`)\n",
    "- **Modeling and preprocessing** (`sklearn`)\n",
    "\n",
    "We'll also be using the **scikit-learn** machine learning library throughout this notebook — make sure to check the documentation links provided at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb394df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing and modeling tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7451e1-3a54-4e3f-8bc7-d416d529a1ba",
   "metadata": {},
   "source": [
    "## 📊 Step 1: Load the Data\n",
    "\n",
    "The dataset contains customer data from a (fictional) telecom company. Each row is a customer, and each column is an attribute describing that customer’s usage patterns or account features.\n",
    "\n",
    "You are given a **train set** and a **test set** (pre-split). This setup simulates a situation where you're developing a model using historical data and then testing how well it might generalize to new customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6f94d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# ✍️ Load the the training set into df_train and the test set into df_test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b179da29",
   "metadata": {},
   "source": [
    "## 🎯 Step 2: Understand the Problem — Classification\n",
    "This is a **binary classification problem**: each customer either **churns** (`Churn = Yes`) or **stays** (`Churn = No`).\n",
    "\n",
    "**Class imbalance:** Only ~14% of customers are churners. This will affect how we train and evaluate the model.\n",
    "\n",
    "📌 We'll address this through evaluation metrics like precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afafe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "# ✍️ Split df_train into X_train and y_train; do the same for df_test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c87e08-9b46-489e-a602-6c041dcfa375",
   "metadata": {},
   "source": [
    "## 🧹 Step 3: Preprocessing — Manual (Leakage-Free)\n",
    "\n",
    "Preprocessing is where most real-world ML effort goes. We must:\n",
    "1. Encode **categorical** features numerically (for the model).\n",
    "2. Scale **numerical** features to help model convergence.\n",
    "3. Avoid **data leakage**: Never use information from the test set to preprocess the training set!\n",
    "\n",
    "📌 We'll handle categorical and numeric features **separately** and then combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5337f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature types\n",
    "# ✍️ Create a list variable named categorical_features, containing the names of all categorical feature columns.\n",
    "\n",
    "# ✍️ Create a list variable named numeric_features, containing the names of all numeric feature columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9987f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual preprocessing\n",
    "# ✍️ Fit a OneHotEncoder to the training set, but apply it to both the training and test set!\n",
    "# 💡 Hint: Recall the difference between the `fit`, `transform` and `fit_transform` methods.\n",
    "# ❗ Important! Use `sparse=False` or `sparse_output=False` (depending on the version of sklearn you're using)\n",
    "# when initializing your OneHotEncoder! It will be important later!\n",
    "\n",
    "\n",
    "# ✍️ Now fit a StandardScaler to the training set, but apply it to both the training and test set!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6090c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now combine the numerical and categorical features:\n",
    "# ℹ️ This wouldn't have worked without `sparse=False`!\n",
    "X_train_final = np.hstack([X_train_num, X_train_cat])\n",
    "X_test_final = np.hstack([X_test_num, X_test_cat])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40a93cd-b574-4fc2-84c4-65f181d24109",
   "metadata": {},
   "source": [
    "🎓 **Why this matters**:\n",
    "\n",
    "- **StandardScaler** rescales features to mean 0 and standard deviation 1, making tree-based models faster and more robust.\n",
    "  - [📘 StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "\n",
    "- **OneHotEncoder** converts categorical variables into binary vectors.\n",
    "  - [📘 OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "\n",
    "✍️ **Your Turn**: Combine the categorical and numeric processed arrays with `np.hstack(...)` to get `X_train_final` and `X_test_final`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b376e2e-6416-4169-97b7-a3357a060f4a",
   "metadata": {},
   "source": [
    "## 🌲 Step 4: Random Forest Training\n",
    "\n",
    "**Random Forests** are one of the most versatile and powerful classifiers, especially for tabular data.\n",
    "\n",
    "They are:\n",
    "- **Ensemble models** made of many decision trees.\n",
    "- Resistant to overfitting thanks to randomness.\n",
    "- Handle both categorical (after encoding) and numeric features well.\n",
    "\n",
    "[📘 RandomForestClassifier Docs](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6247024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest classifier\n",
    "# ✍️ Create a new RandomForestClassifier and assign it the a variable named `model`.\n",
    "# ✍️ Fit your RandomForestClassifier in the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a802c250-cf8c-405a-a78c-abeaca4e0a54",
   "metadata": {},
   "source": [
    "## 📊 Step 5: Evaluation on Held-Out Test Set\n",
    "\n",
    "To estimate how well your model might perform in production, use a **test set that the model hasn’t seen**.\n",
    "\n",
    "Use:\n",
    "- [📘 accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "- [📘 classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)\n",
    "\n",
    "📌 **Reminder**: With class imbalance, accuracy can be misleading. Look at **precision, recall, and F1 score** as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d74ea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "# ✍️ Save the prediction of your model on the test set to `y_pred`.\n",
    "\n",
    "# ✍️ Print the accuracy score your model got.\n",
    "\n",
    "# ✍️ Print a classification report for your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa7813b-5da5-43b2-8e69-aafa9888fd26",
   "metadata": {},
   "source": [
    "## 🔁 Step 6: Use Pipelines\n",
    "\n",
    "Manual preprocessing is error-prone. Use [📘 Pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) to:\n",
    "\n",
    "- Bundle preprocessing and modeling together.\n",
    "- Reduce leakage risk.\n",
    "- Improve reproducibility.\n",
    "\n",
    "We'll use [📘 ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) to apply different preprocessing to numeric and categorical features.\n",
    "\n",
    "📚 Read more:\n",
    "- [📘 sklearn Pipelines Tutorial](https://scikit-learn.org/stable/modules/compose.html)\n",
    "- [📝 Blog Post](https://towardsdatascience.com/how-to-use-columntransformer-for-numerical-and-categorical-data-6d8cbd25f4b6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b41ebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn pipeline\n",
    "# ✍️ Create a ColumnTransformer object, made up of a StandardScaler followed by a OneHotEncoder object.\n",
    "#❗ Important! Remember to use `sparse=False` or `sparse_output=False`.\n",
    "\n",
    "\n",
    "# ✍️ Now create a Pipeline object made up of your column transformer object and a RandomForestClassifier object.\n",
    "\n",
    "\n",
    "# ✍️ Fit your pipeline object on the training set, then predict results for the test set\n",
    "# and produce a classification report.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea32585-1822-4f94-a0fa-16938544de12",
   "metadata": {},
   "source": [
    "## 🔍 Step 7: Manual Hyperparameter Tuning\n",
    "\n",
    "To make better models, tune key hyperparameters. We’ll try different values for `max_depth`.\n",
    "\n",
    "We'll use a **train-validation-test** split:\n",
    "- **Train**: build models\n",
    "- **Validation**: compare models\n",
    "- **Test**: final performance estimate\n",
    "\n",
    "[📘 train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9426854-68c6-4ce4-958d-1c13d3f0b7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's unite the training and validation sets into one big set:\n",
    "df_full = pd.concat([df_train, df_test], ignore_index=True)\n",
    "X = df_full.drop(\"Churn\", axis=1)\n",
    "y = df_full[\"Churn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9540e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-validation-test split for tuning\n",
    "# We use train_test_split to get a three-way split: Firstg into trainval and test;\n",
    "# then we split trainval into train and validation.\n",
    "X_trainval, X_test_final, y_trainval, y_test_final = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.25, random_state=42, stratify=y_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8e0efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual hyperparameter tuning\n",
    "# ✍️ Manually find the best value for the max_depth hyperparameter out of [5, 10, 15, 20]:\n",
    "# 💡 Hint: Use a for loop to iterate over a list of possible values. Then, for each such value,\n",
    "# create a new pipeline with the appropriate value for RandomForestClassifier (you DON'T have to\n",
    "# create your ColumnTransformer again; use the same object you've already created), fit it on the\n",
    "# training set and evaluate it on the **validation set**! Do NOT touch the test set in this loop!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9f43ee-f6b1-42b1-8c3f-cb27ed56ac08",
   "metadata": {},
   "source": [
    "## ✅ Step 8: Final Evaluation and Deployment-Ready Model\n",
    "\n",
    "After choosing the best hyperparameter from the validation set, retrain your model on **train+val** and evaluate on **test**.\n",
    "\n",
    "Then, retrain a final version on **all labeled data** to use in production.\n",
    "\n",
    "You can save the model for future use using:\n",
    "[📘 sklearn model persistence](https://scikit-learn.org/stable/model_persistence.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d6c4cf-0a25-4c4b-8b1f-d09422ebfbf4",
   "metadata": {},
   "source": [
    "### 8.1: Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f533c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model evaluation\n",
    "# ✍️ Now create the pipeline again, now with max_depth set to the value which showed the\n",
    "# best results. Then, fit it on the **X_trainval** set, and predict on the test set.\n",
    "# What is the model performance? That's our estimate for future performamce.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd229a26-c1c1-44b5-bee1-ca62333a908d",
   "metadata": {},
   "source": [
    "### 💬 Answer the following questions:\n",
    "- Are there more false positives or false negatives?\n",
    "- Which mistake is more costly in your business scenario?\n",
    "- Should you tune the model to favor precision or recall?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd23b69b-348f-4edf-ad33-6443003be3a6",
   "metadata": {},
   "source": [
    "### 🧪 Sub-Task: Plot the Confusion Matrix\n",
    "\n",
    "The **confusion matrix** helps you visually assess where your model is getting things right or wrong.\n",
    "\n",
    "It breaks predictions into:\n",
    "- **True Positives (TP)**: correctly predicted churners.\n",
    "- **True Negatives (TN)**: correctly predicted non-churners.\n",
    "- **False Positives (FP)**: predicted churn but customer stayed.\n",
    "- **False Negatives (FN)**: predicted stay but customer churned.\n",
    "\n",
    "This can help you **understand your model's trade-offs**, especially when classifying rare events (like churn).\n",
    "\n",
    "📘 Documentation:\n",
    "- [sklearn.metrics.confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "- [sklearn.metrics.ConfusionMatrixDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html)\n",
    "\n",
    "✍️ **Your task**: Use `ConfusionMatrixDisplay.from_predictions(...)` to plot the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c1638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "# ✍️ Use ConfusionMatrixDisplay.from_predictions()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4bb79c-4c92-484f-b69f-1e8691bb4671",
   "metadata": {},
   "source": [
    "### 8.2: Train the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b38234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final product-ready model\n",
    "# ✍️ Now create the pipeline again, one last time. Fit it on the entire dataset (X and y variables).\n",
    "# That's it! You're done! Great job!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

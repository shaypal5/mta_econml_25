{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Fraud Detection\n",
    "## From Perceptron to Multi-Layer Networks\n",
    "\n",
    "This notebook implements neural networks of increasing complexity for fraud detection, building upon our understanding of logistic regression. We'll follow a structured journey from simple perceptrons to multi-layer neural networks, with a focus on fraud detection applications.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Data Preparation](#1-setup-and-data-preparation)\n",
    "2. [The Perceptron for Fraud Detection](#2-the-perceptron-for-fraud-detection)\n",
    "3. [Decision Boundary Visualization](#3-decision-boundary-visualization)\n",
    "4. [Multi-Layer Perceptron with Keras](#4-multi-layer-perceptron-with-keras)\n",
    "5. [Training and Optimization](#5-training-and-optimization)\n",
    "6. [Model Evaluation for Fraud](#6-model-evaluation-for-fraud)\n",
    "7. [Balancing Complexity and Interpretability](#7-balancing-complexity-and-interpretability)\n",
    "8. [Performance Comparison](#8-performance-comparison)\n",
    "9. [Conclusion and Future Directions](#9-conclusion-and-future-directions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Preparation\n",
    "\n",
    "Let's first set up our environment and prepare the fraud dataset. We'll reuse the preprocessing steps from our logistic regression implementation to ensure fair comparison between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_curve, roc_auc_score, precision_recall_curve, average_precision_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers, callbacks\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"deep\")\n",
    "plt.rcParams.update({'font.size': 14, 'axes.labelsize': 14, 'xtick.labelsize': 12, \n",
    "                     'ytick.labelsize': 12, 'legend.fontsize': 12})\n",
    "\n",
    "# Check for GPU availability (for Keras)\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU Available: \", len(tf.config.list_physical_devices('GPU')) > 0)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fraud dataset\n",
    "df = pd.read_csv('fraud_test_head10.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "We'll apply the same preprocessing steps used for logistic regression to ensure a fair comparison between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({'Missing Values': missing_values, \n",
    "                           'Percentage': missing_percent})\n",
    "print(\"Missing values summary:\")\n",
    "print(missing_df[missing_df['Missing Values'] > 0])\n",
    "\n",
    "# Handle missing values (creating flags and imputing)\n",
    "numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "numeric_features.remove('fraud_bool')  # Remove target variable\n",
    "\n",
    "for feature in numeric_features:\n",
    "    if df[feature].isnull().sum() > 0:\n",
    "        df[f'{feature}_missing'] = df[feature].isnull().astype(int)\n",
    "        df[feature].fillna(df[feature].median(), inplace=True)\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Prepare features and target\n",
    "X = df_encoded.drop('fraud_bool', axis=1)\n",
    "y = df_encoded['fraud_bool']\n",
    "\n",
    "# Split data with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Testing set shape: {X_test_scaled.shape}\")\n",
    "print(f\"Fraud percentage in training set: {y_train.mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Perceptron for Fraud Detection\n",
    "\n",
    "The perceptron is the simplest form of a neural network - a single artificial neuron. It takes weighted inputs, applies a step function, and produces a binary output. Despite its simplicity, it helps us understand the foundation of neural networks.\n",
    "\n",
    "### Theoretical Background\n",
    "\n",
    "The perceptron algorithm works as follows:\n",
    "1. Initialize weights randomly\n",
    "2. For each input sample:\n",
    "   - Calculate the weighted sum: $z = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n$\n",
    "   - Apply step function: $output = 1$ if $z > 0$ else $0$\n",
    "   - Update weights if prediction is incorrect: $w_i = w_i + \\alpha(y - \\hat{y})x_i$\n",
    "3. Repeat until convergence or maximum iterations\n",
    "\n",
    "Unlike logistic regression which outputs probabilities using the sigmoid function, the perceptron produces binary outputs using a step function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a perceptron for fraud detection\n",
    "perceptron = Perceptron(max_iter=1000, random_state=42, alpha=0.0001)\n",
    "perceptron.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_perceptron = perceptron.predict(X_test_scaled)\n",
    "\n",
    "# Display basic metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_perceptron)\n",
    "precision = precision_score(y_test, y_pred_perceptron)\n",
    "recall = recall_score(y_test, y_pred_perceptron)\n",
    "f1 = f1_score(y_test, y_pred_perceptron)\n",
    "\n",
    "print(\"Perceptron Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_perceptron)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Legitimate', 'Fraud'],\n",
    "            yticklabels=['Legitimate', 'Fraud'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Perceptron Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Perceptron's Limitations\n",
    "\n",
    "The perceptron has several key limitations for fraud detection:\n",
    "\n",
    "1. **Binary output**: The perceptron only outputs 0 or 1, not probabilities. This makes it difficult to adjust thresholds for different business needs.\n",
    "\n",
    "2. **Linear decision boundary**: Like logistic regression, the perceptron can only create linear decision boundaries, limiting its ability to capture complex fraud patterns.\n",
    "\n",
    "3. **No built-in regularization**: The standard perceptron lacks regularization mechanisms, making it prone to overfitting.\n",
    "\n",
    "4. **Cannot solve XOR problems**: The perceptron cannot learn non-linearly separable patterns like XOR, which are common in fraud detection (e.g., a transaction might be suspicious if amount is very high OR very low, but not in the middle).\n",
    "\n",
    "Let's examine the perceptron's decision weights to see what it learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine perceptron weights\n",
    "perceptron_weights = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Weight': perceptron.coef_[0]\n",
    "})\n",
    "\n",
    "# Sort by absolute weight value\n",
    "perceptron_weights['Abs_Weight'] = abs(perceptron_weights['Weight'])\n",
    "perceptron_weights_sorted = perceptron_weights.sort_values('Abs_Weight', ascending=False)\n",
    "\n",
    "# Display top weights\n",
    "print(\"Top 10 most important features according to perceptron:\")\n",
    "print(perceptron_weights_sorted.head(10))\n",
    "\n",
    "# Visualize top weights\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_weights = perceptron_weights_sorted.head(15)\n",
    "colors = ['red' if x < 0 else 'green' for x in top_weights['Weight']]\n",
    "plt.barh(top_weights['Feature'], top_weights['Weight'], color=colors)\n",
    "plt.xlabel('Weight Value')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top 15 Features by Importance in Perceptron Model')\n",
    "plt.axvline(x=0, color='black', linestyle='-')\n",
    "plt.grid(axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decision Boundary Visualization\n",
    "\n",
    "To better understand the differences between logistic regression and the perceptron, let's visualize their decision boundaries on a simplified 2D projection of our data. This will help illustrate the limitations of linear models for complex fraud patterns.\n",
    "\n",
    "We'll use Principal Component Analysis (PCA) to reduce our dataset to 2 dimensions while preserving as much variance as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Create a 2D projection of our data using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train_2d = pca.fit_transform(X_train_scaled)\n",
    "X_test_2d = pca.transform(X_test_scaled)\n",
    "\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.2f}\")\n",
    "\n",
    "# Train both models on the 2D data\n",
    "# Logistic Regression\n",
    "fraud_ratio = y_train.mean()\n",
    "class_weights = {0: fraud_ratio, 1: 1-fraud_ratio}\n",
    "logistic_2d = LogisticRegression(class_weight=class_weights, random_state=42).fit(X_train_2d, y_train)\n",
    "\n",
    "# Perceptron\n",
    "perceptron_2d = Perceptron(max_iter=1000, random_state=42).fit(X_train_2d, y_train)\n",
    "\n",
    "# Create a mesh grid for visualization\n",
    "def plot_decision_boundary(X, y, models, model_names):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                         np.arange(y_min, y_max, 0.1))\n",
    "    \n",
    "    f, axarr = plt.subplots(1, len(models), figsize=(15, 6))\n",
    "    \n",
    "    for idx, (model, name) in enumerate(zip(models, model_names)):\n",
    "        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        axarr[idx].contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "        scatter = axarr[idx].scatter(X[:, 0], X[:, 1], c=y, \n",
    "                           s=20, edgecolor='k', cmap='coolwarm')\n",
    "        axarr[idx].set_title(f'{name} Decision Boundary')\n",
    "        axarr[idx].set_xlabel('Principal Component 1')\n",
    "        axarr[idx].set_ylabel('Principal Component 2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundaries\n",
    "plot_decision_boundary(X_train_2d, y_train, \n",
    "                     [logistic_2d, perceptron_2d],\n",
    "                     ['Logistic Regression', 'Perceptron'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Decision Boundaries\n",
    "\n",
    "The visualization above illustrates several important points:\n",
    "\n",
    "1. **Linear Limitations**: Both logistic regression and the perceptron create linear decision boundaries, which may not capture the complex patterns in fraud data.\n",
    "\n",
    "2. **Subtle Differences**: While mathematically similar, logistic regression and the perceptron can produce slightly different boundaries due to their different training objectives (log loss vs. hinge loss).\n",
    "\n",
    "3. **Dimensionality Reduction Trade-off**: By reducing to 2D for visualization, we've lost information (as shown by the explained variance ratio). Real fraud patterns likely exist in higher dimensions that cannot be visualized directly.\n",
    "\n",
    "These limitations motivate the need for more flexible models like multi-layer neural networks that can create non-linear decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Layer Perceptron with Keras\n",
    "\n",
    "Now we'll implement a Multi-Layer Perceptron (MLP) using Keras. This will allow us to create non-linear decision boundaries and capture more complex fraud patterns.\n",
    "\n",
    "### What is a Multi-Layer Perceptron?\n",
    "\n",
    "A Multi-Layer Perceptron (MLP) is a feedforward neural network with one or more hidden layers between the input and output layers. The key improvements over a single perceptron include:\n",
    "\n",
    "1. **Non-linear activation functions**: Hidden layers use activation functions like ReLU or tanh to introduce non-linearity.\n",
    "\n",
    "2. **Multiple layers**: Each layer can learn different representations of the data, with deeper layers capturing more abstract concepts.\n",
    "\n",
    "3. **Backpropagation**: MLPs are trained using backpropagation, which efficiently calculates gradients for all weights in the network.\n",
    "\n",
    "### Designing a Network Architecture for Fraud\n",
    "\n",
    "For fraud detection, we need to consider several factors when designing our network:\n",
    "\n",
    "1. **Input layer**: One node per feature (same as our scaled feature count)\n",
    "\n",
    "2. **Hidden layers**: Usually 1-3 hidden layers is sufficient for tabular data like our fraud dataset\n",
    "\n",
    "3. **Hidden layer size**: Common heuristics include:\n",
    "   - Mean of input and output size\n",
    "   - 2/3 the size of the input layer\n",
    "   - Power of 2 (e.g., 32, 64, 128) for computational efficiency\n",
    "\n",
    "4. **Output layer**: Single node with sigmoid activation for binary classification\n",
    "\n",
    "5. **Regularization**: Dropout and/or L2 regularization to prevent overfitting, especially important with imbalanced data\n",
    "\n",
    "Let's implement this model with Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create MLP model\n",
    "def create_mlp_model(input_dim, hidden_layers=[64, 32], dropout_rate=0.3, l2_strength=0.01):\n",
    "    \"\"\"Create a multi-layer perceptron model for fraud detection\"\"\"\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(layers.Input(shape=(input_dim,)))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for units in hidden_layers:\n",
    "        model.add(layers.Dense(\n",
    "            units=units,\n",
    "            activation='relu',\n",
    "            kernel_regularizer=regularizers.l2(l2_strength)\n",
    "        ))\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer (sigmoid for binary classification)\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "mlp_model = create_mlp_model(input_dim)\n",
    "\n",
    "# Compile model with class weights (for imbalanced data)\n",
    "# Calculate class weights (inverse of frequency)\n",
    "fraud_ratio = y_train.mean()\n",
    "class_weight = {0: fraud_ratio, 1: 1-fraud_ratio}\n",
    "\n",
    "mlp_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        keras.metrics.Precision(),\n",
    "        keras.metrics.Recall(),\n",
    "        keras.metrics.AUC()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the MLP Architecture\n",
    "\n",
    "Let's analyze the MLP architecture we've created:\n",
    "\n",
    "1. **Input Layer**: Takes our scaled features as input\n",
    "\n",
    "2. **Hidden Layers**: Two hidden layers with 64 and 32 neurons respectively, using ReLU activation for non-linearity\n",
    "\n",
    "3. **Regularization**: \n",
    "   - Dropout (30%) to prevent overfitting by randomly disabling neurons during training\n",
    "   - L2 regularization (weight decay) to prevent large weights\n",
    "\n",
    "4. **Output Layer**: Single neuron with sigmoid activation, outputting a probability between 0 and 1\n",
    "\n",
    "5. **Loss Function**: Binary cross-entropy, appropriate for binary classification problems\n",
    "\n",
    "6. **Optimizer**: Adam, which adapts the learning rate and includes momentum\n",
    "\n",
    "7. **Class Weights**: Handling class imbalance by weighting the minority class (fraud) more heavily\n",
    "\n",
    "This architecture balances complexity (ability to learn non-linear patterns) with regularization (to prevent overfitting), making it suitable for fraud detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training and Optimization\n",
    "\n",
    "Now we'll train our MLP model and implement several techniques to optimize its performance, including:\n",
    "\n",
    "1. **Early stopping**: To prevent overfitting by monitoring validation loss\n",
    "2. **Learning rate scheduling**: To adjust the learning rate during training\n",
    "3. **Validation set**: To monitor performance on unseen data during training\n",
    "\n",
    "These techniques are particularly important for fraud detection, where overfitting to the training data can lead to poor generalization to new fraud patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a validation set from the training data\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_scaled, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# Set up callbacks\n",
    "callbacks = [\n",
    "    # Early stopping to prevent overfitting\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    # Learning rate reduction when plateau is reached\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=0.00001\n",
    "    ),\n",
    "    # TensorBoard for visualization (optional)\n",
    "    # callbacks.TensorBoard(log_dir='./logs')\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = mlp_model.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    epochs=100,  # We'll use early stopping to determine actual epochs\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    class_weight=class_weight,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training & validation loss and metrics\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(history.history['loss'], label='Training Loss')\n",
    "    ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax1.set_title('Model Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot metrics\n",
    "    metrics = ['precision', 'recall', 'auc']\n",
    "    for metric in metrics:\n",
    "        ax2.plot(history.history[metric], label=f'Training {metric.capitalize()}')\n",
    "        ax2.plot(history.history[f'val_{metric}'], label=f'Validation {metric.capitalize()}')\n",
    "    \n",
    "    ax2.set_title('Model Metrics')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Analysis\n",
    "\n",
    "The training plots reveal important insights about our model's learning process:\n",
    "\n",
    "1. **Convergence**: The loss curves show whether the model converged to a stable solution\n",
    "\n",
    "2. **Overfitting**: The gap between training and validation loss indicates potential overfitting\n",
    "\n",
    "3. **Metric Trajectory**: The precision, recall, and AUC curves show how the model's fraud detection capabilities improved during training\n",
    "\n",
    "4. **Early Stopping Effect**: We can see where early stopping prevented overfitting by stopping training when validation loss stopped improving\n",
    "\n",
    "For fraud detection, it's particularly important to monitor metrics like precision and recall during training, as accuracy alone can be misleading with imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation for Fraud\n",
    "\n",
    "Now, let's thoroughly evaluate our MLP model on the test dataset. For fraud detection, we need to consider performance metrics beyond accuracy, focusing on precision, recall, and business impact metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on test set\n",
    "y_pred_proba_mlp = mlp_model.predict(X_test_scaled).flatten()\n",
    "y_pred_mlp = (y_pred_proba_mlp >= 0.5).astype(int)\n",
    "\n",
    "# Basic classification metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_mlp)\n",
    "precision = precision_score(y_test, y_pred_mlp)\n",
    "recall = recall_score(y_test, y_pred_mlp)\n",
    "f1 = f1_score(y_test, y_pred_mlp)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba_mlp)\n",
    "avg_precision = average_precision_score(y_test, y_pred_proba_mlp)\n",
    "\n",
    "print(\"MLP Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_mlp)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Legitimate', 'Fraud'],\n",
    "            yticklabels=['Legitimate', 'Fraud'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('MLP Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and PR curves\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test, y_pred_proba_mlp)\n",
    "precision_curve, recall_curve, thresholds_pr = precision_recall_curve(y_test, y_pred_proba_mlp)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr, tpr, lw=2, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(recall_curve, precision_curve, lw=2, \n",
    "         label=f'PR Curve (AP = {avg_precision:.3f})')\n",
    "plt.axhline(y=y_test.mean(), color='k', linestyle='--', \n",
    "            label=f'Baseline (Fraud Rate = {y_test.mean():.3f})')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Optimization for Business Impact\n",
    "\n",
    "For fraud detection, selecting the optimal threshold is a business decision that balances false positives and false negatives. Let's implement a threshold optimization approach based on business costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold optimization with business impact\n",
    "thresholds = np.arange(0.1, 1.0, 0.05)\n",
    "threshold_metrics = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_custom = (y_pred_proba_mlp >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision_custom = precision_score(y_test, y_pred_custom)\n",
    "    recall_custom = recall_score(y_test, y_pred_custom)\n",
    "    f1_custom = f1_score(y_test, y_pred_custom)\n",
    "    \n",
    "    # Business metrics (simplified)\n",
    "    # Assume: \n",
    "    # - Each fraudulent transaction costs $1000 on average\n",
    "    # - Each manual review costs $10\n",
    "    # - Each false positive causes $2 in customer friction\n",
    "    \n",
    "    true_positives = ((y_pred_custom == 1) & (y_test == 1)).sum()\n",
    "    false_positives = ((y_pred_custom == 1) & (y_test == 0)).sum()\n",
    "    false_negatives = ((y_pred_custom == 0) & (y_test == 1)).sum()\n",
    "    \n",
    "    fraud_savings = true_positives * 1000\n",
    "    review_cost = (true_positives + false_positives) * 10\n",
    "    friction_cost = false_positives * 2\n",
    "    missed_fraud_cost = false_negatives * 1000\n",
    "    \n",
    "    net_value = fraud_savings - review_cost - friction_cost - missed_fraud_cost\n",
    "    \n",
    "    threshold_metrics.append({\n",
    "        'Threshold': threshold,\n",
    "        'Precision': precision_custom,\n",
    "        'Recall': recall_custom,\n",
    "        'F1': f1_custom,\n",
    "        'TP': true_positives,\n",
    "        'FP': false_positives,\n",
    "        'FN': false_negatives,\n",
    "        'Net Value': net_value\n",
    "    })\n",
    "\n",
    "threshold_df = pd.DataFrame(threshold_metrics)\n",
    "\n",
    "# Plot metrics across thresholds\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(threshold_df['Threshold'], threshold_df['Precision'], marker='o', label='Precision')\n",
    "plt.plot(threshold_df['Threshold'], threshold_df['Recall'], marker='s', label='Recall')\n",
    "plt.plot(threshold_df['Threshold'], threshold_df['F1'], marker='^', label='F1 Score')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Classification Metrics by Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(threshold_df['Threshold'], threshold_df['Net Value'], marker='D', color='purple')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Net Business Value ($)')\n",
    "plt.title('Business Impact by Threshold')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal threshold based on business value\n",
    "optimal_threshold_idx = threshold_df['Net Value'].idxmax()\n",
    "optimal_threshold = threshold_df.loc[optimal_threshold_idx, 'Threshold']\n",
    "optimal_metrics = threshold_df.loc[optimal_threshold_idx]\n",
    "\n",
    "print(f\"Optimal Threshold: {optimal_threshold:.2f}\")\n",
    "print(f\"At this threshold:\")\n",
    "print(f\"  Precision: {optimal_metrics['Precision']:.4f}\")\n",
    "print(f\"  Recall: {optimal_metrics['Recall']:.4f}\")\n",
    "print(f\"  F1 Score: {optimal_metrics['F1']:.4f}\")\n",
    "print(f\"  Net Business Value: ${optimal_metrics['Net Value']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Balancing Complexity and Interpretability\n",
    "\n",
    "One of the key challenges with neural networks for fraud detection is the trade-off between model complexity (which can improve performance) and interpretability (which is crucial for business and regulatory requirements).\n",
    "\n",
    "Let's explore methods to help interpret our MLP model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial Dependence Plots (PDPs)\n",
    "from sklearn.inspection import partial_dependence, plot_partial_dependence\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# First we need to train a scikit-learn MLPClassifier (since Keras models aren't compatible with PDPs)\n",
    "sklearn_mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.01,  # L2 regularization\n",
    "    batch_size=32,\n",
    "    max_iter=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "sklearn_mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Find most important features from our previous analysis\n",
    "# In a real scenario, we'd have identified these from feature importance\n",
    "important_features = [0, 1, 2, 3, 4]  # Example indices of important features\n",
    "\n",
    "# Create PDPs for important features\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "plot_partial_dependence(\n",
    "    sklearn_mlp, X_train_scaled, important_features,\n",
    "    feature_names=X_train.columns,\n",
    "    n_cols=3,\n",
    "    ax=ax\n",
    ")\n",
    "plt.suptitle('Partial Dependence Plots for Top Features', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Values for Local Interpretability\n",
    "\n",
    "While Partial Dependence Plots help understand the global behavior of the model, SHAP (SHapley Additive exPlanations) values can provide local interpretability for individual predictions. This is particularly valuable in fraud detection, where we may need to explain why a specific transaction was flagged as fraudulent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install shap if not already installed\n",
    "# !pip install shap\n",
    "\n",
    "import shap\n",
    "\n",
    "# Create a Keras model explainer\n",
    "# We'll use a smaller sample for computational efficiency\n",
    "background = X_train_scaled[:100]  # Background dataset for the explainer\n",
    "test_sample = X_test_scaled[:10]   # Sample of test instances to explain\n",
    "\n",
    "# Create explainer\n",
    "explainer = shap.KernelExplainer(mlp_model.predict, background)\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer.shap_values(test_sample)\n",
    "\n",
    "# Visualize SHAP values\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, test_sample, feature_names=X_train.columns)\n",
    "plt.title('SHAP Values for Neural Network Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Force plot for a single prediction (most confident fraud prediction)\n",
    "y_pred_sample = mlp_model.predict(test_sample).flatten()\n",
    "most_confident_idx = np.argmax(y_pred_sample)\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "shap.force_plot(explainer.expected_value, shap_values[most_confident_idx, :], \n",
    "                X_test_scaled[most_confident_idx, :], \n",
    "                feature_names=list(X_train.columns), matplotlib=True)\n",
    "plt.title(f'SHAP Force Plot for High Confidence Prediction ({y_pred_sample[most_confident_idx]:.4f})')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretability Discussion\n",
    "\n",
    "The interpretability tools we've explored reveal important insights about our neural network model:\n",
    "\n",
    "1. **Feature Importance**: SHAP values identify which features have the most influence on the model's predictions, both globally and for individual cases.\n",
    "\n",
    "2. **Feature Effects**: Partial Dependence Plots show how changes in feature values affect the predicted probability of fraud, revealing non-linear relationships.\n",
    "\n",
    "3. **Individual Explanations**: For specific predictions, SHAP force plots provide a detailed breakdown of how each feature contributes to the final prediction.\n",
    "\n",
    "These tools help address the \"black box\" concern of neural networks, making them more acceptable for fraud detection applications where explanations may be required for regulatory compliance or customer inquiries.\n",
    "\n",
    "However, there remains a fundamental trade-off between model complexity and interpretability. In some regulated environments, simpler models like logistic regression might still be preferred for their inherent interpretability, despite potentially lower performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Comparison\n",
    "\n",
    "Now let's compare the performance of our three models: Logistic Regression, Perceptron, and MLP. We'll implement logistic regression to complete the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression model\n",
    "logistic_model = LogisticRegression(\n",
    "    penalty='l2',\n",
    "    C=1.0,\n",
    "    solver='liblinear',\n",
    "    class_weight=class_weight,\n",
    "    random_state=42,\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "logistic_model.fit(X_train_scaled, y_train)\n",
    "y_pred_proba_lr = logistic_model.predict_proba(X_test_scaled)[:, 1]\n",
    "y_pred_lr = logistic_model.predict(X_test_scaled)\n",
    "\n",
    "# Collect all results\n",
    "models = ['Logistic Regression', 'Perceptron', 'MLP']\n",
    "y_pred_list = [y_pred_lr, y_pred_perceptron, y_pred_mlp]\n",
    "y_proba_list = [y_pred_proba_lr, None, y_pred_proba_mlp]  # Perceptron doesn't output probabilities\n",
    "\n",
    "# Calculate metrics for each model\n",
    "results = []\n",
    "\n",
    "for i, model_name in enumerate(models):\n",
    "    y_pred = y_pred_list[i]\n",
    "    y_proba = y_proba_list[i]\n",
    "    \n",
    "    # Basic metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # AUC and Average Precision (only for models with probability outputs)\n",
    "    if y_proba is not None:\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "        ap = average_precision_score(y_test, y_proba)\n",
    "    else:\n",
    "        auc = np.nan\n",
    "        ap = np.nan\n",
    "    \n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1 Score': f1,\n",
    "        'AUC': auc,\n",
    "        'Avg Precision': ap\n",
    "    })\n",
    "\n",
    "# Create comparison dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Model Comparison:\")\n",
    "print(results_df.set_index('Model').round(4))\n",
    "\n",
    "# Visualize comparison\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "for i, model_name in enumerate(models):\n",
    "    values = [results_df.loc[results_df['Model'] == model_name, metric].values[0] for metric in metrics]\n",
    "    plt.bar(x + i*width, values, width, label=model_name)\n",
    "\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(x + width, metrics)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, model_name in enumerate(models):\n",
    "    values = [results_df.loc[results_df['Model'] == model_name, metric].values[0] for metric in metrics]\n",
    "    for j, v in enumerate(values):\n",
    "        plt.text(j + i*width, v + 0.01, f'{v:.3f}', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve Comparison\n",
    "\n",
    "Let's also compare the ROC curves for the models that produce probability outputs (Logistic Regression and MLP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Logistic Regression\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_proba_lr)\n",
    "auc_lr = roc_auc_score(y_test, y_pred_proba_lr)\n",
    "plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {auc_lr:.3f})')\n",
    "\n",
    "# MLP\n",
    "fpr_mlp, tpr_mlp, _ = roc_curve(y_test, y_pred_proba_mlp)\n",
    "auc_mlp = roc_auc_score(y_test, y_pred_proba_mlp)\n",
    "plt.plot(fpr_mlp, tpr_mlp, label=f'MLP (AUC = {auc_mlp:.3f})')\n",
    "\n",
    "# Random classifier\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison Analysis\n",
    "\n",
    "Our comparison reveals several important insights:\n",
    "\n",
    "1. **Accuracy vs. Fraud-Specific Metrics**: While overall accuracy might be similar across models, fraud-specific metrics like precision, recall, and F1 score show larger differences, highlighting the importance of the right evaluation metrics.\n",
    "\n",
    "2. **MLP Advantages**: The MLP generally outperforms the other models in most metrics, particularly in AUC and average precision, indicating better ability to rank fraudulent cases correctly.\n",
    "\n",
    "3. **Perceptron Limitations**: The perceptron performs worse than both logistic regression and MLP, confirming its limitations for complex tasks like fraud detection.\n",
    "\n",
    "4. **Probability Calibration**: The comparison of ROC curves shows the advantage of models that produce well-calibrated probabilities, which is crucial for threshold optimization.\n",
    "\n",
    "These results demonstrate the natural progression from simpler models (perceptron) to more powerful ones (MLP), with each step bringing improved performance for fraud detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion and Future Directions\n",
    "\n",
    "We've completed our journey from the perceptron to multi-layer neural networks for fraud detection. Let's summarize our key findings and discuss future directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "\n",
    "1. **Model Evolution**: We observed a clear progression in capabilities from the perceptron to logistic regression to MLPs, with each step adding more capacity to model complex fraud patterns.\n",
    "\n",
    "2. **Non-linear Advantage**: The MLP's ability to learn non-linear decision boundaries provides a significant advantage for fraud detection, where patterns often involve complex combinations of features.\n",
    "\n",
    "3. **Interpretability Tools**: While neural networks are often considered \"black boxes,\" tools like SHAP values and partial dependence plots can help explain their predictions, addressing a key concern for fraud applications.\n",
    "\n",
    "4. **Business Impact Focus**: Our threshold optimization approach demonstrated how to align model decisions with business objectives, moving beyond pure technical metrics.\n",
    "\n",
    "5. **Architectural Considerations**: The MLP architecture with appropriate regularization (dropout, L2) and training techniques (early stopping, learning rate scheduling) effectively handles the challenges of fraud detection, including class imbalance.\n",
    "\n",
    "### Future Directions\n",
    "\n",
    "Several promising directions could further enhance neural network approaches for fraud detection:\n",
    "\n",
    "1. **More Advanced Architectures**:\n",
    "   - **Recurrent Neural Networks (RNNs)**: For capturing sequential patterns in transaction history\n",
    "   - **Graph Neural Networks (GNNs)**: For modeling relationships between accounts, devices, and IP addresses\n",
    "   - **AutoEncoders**: For unsupervised anomaly detection to complement supervised classification\n",
    "\n",
    "2. **Feature Engineering**:\n",
    "   - **Automated Feature Engineering**: Using techniques like deep feature synthesis to create more predictive features\n",
    "   - **Representation Learning**: Having the network automatically learn useful representations from raw data\n",
    "   - **Time-based Features**: Incorporating more sophisticated temporal patterns and cyclical features\n",
    "\n",
    "3. **Deployment Considerations**:\n",
    "   - **Model Monitoring**: Implementing systems to detect concept drift as fraud patterns evolve\n",
    "   - **Ensemble Approaches**: Combining multiple models (including both neural networks and traditional methods) for better overall performance\n",
    "   - **Real-time Scoring**: Optimizing models for production environments where low latency is critical\n",
    "\n",
    "4. **Ethical and Regulatory Considerations**:\n",
    "   - **Fairness Assessment**: Evaluating and addressing potential biases in fraud detection models\n",
    "   - **Explainability Frameworks**: Developing more comprehensive approaches to model explanations for regulatory compliance\n",
    "   - **Privacy-Preserving Techniques**: Implementing methods like federated learning that enhance privacy while maintaining detection performance\n",
    "\n",
    "### Concluding Remarks\n",
    "\n",
    "Neural networks offer powerful capabilities for fraud detection, with the ability to learn complex patterns that simpler models miss. By understanding the progression from perceptrons to more sophisticated architectures, we gain insight into both the mathematical foundations and practical applications of these models.\n",
    "\n",
    "The optimal approach for a specific fraud detection scenario depends on various factors, including data characteristics, performance requirements, interpretability needs, and regulatory constraints. By mastering the spectrum of techniques from logistic regression to neural networks, fraud analysts can select and customize the right tools for each unique challenge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}